%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Discussion and Conclusion}
The focus of this work was on understanding DDPMs and conditioning them for the task of reconstructing undersampled MRI. The modeling and loss functions of the DDPM were derived in great detail and a package was created that implements DDPMs from scratch and provides the necessary utilities for efficient training, logging and sampling. Prior work by Lugmayr et al. and Choi et al.~\autocite{lugmayr2022repaint,choi2021ilvr} had introduced a conditioning method for inpainting and image-to-image translation respectively that used the known posterior of the forward process to substitute predicted information with known information in the latent space. Both their works, RePaint and ILVR, were successfully implemented with a model trained on MRI data and resampling for RePaint was equally observed to improve semantics of the reconstruction. Lugmayr et al.'s work was subsequently adapted to the task of MRI reconstruction, but the direct adaptation was shown to produce insufficient reconstructions. In order to avoid image artifacts, like aliasing and ringing, Choi et al.'s filtering was reintroduced in the form of a scheduled Gaussian filter that only conditions the model on low frequencies early in the reverse diffusion process and introduces higher frequencies later. The intuition behind this approach was that low frequencies would carry very little information for high noise variances of the latent space, since this is a general property of natural images. While reconstruction quality failed to improve through the use of filtering, the analysis of outcome variability offered a unique view on the hierarchy of features in a DDPM. As hypothesized, global image features corresponding to low frequencies were determined early in the DDPM, whereas high frequency showed variety until very late in the denoising process.

Using the score-based interpretation of the DDPM and adapting classifier guidance to accomodate other types of data consistency functions proved to be a much more flexible and powerful approach to the conditioning problem. \textit{Loss guidance} performed very well out of the box for the lower accelerations in the range $\approx 3-6$, with high perceptual sample quality and very good MSE scores. For the highest acceleration of $>11$, direct sampling often produced aliasing artifacts, indicating that the final prediction did not correspond to the same distributional mode as the samples used for guidance, which led to frequency mismatch. Using the \textit{long-grained} resampling technique, this issue was resolved and the very high acceleration of $>11$ produced reconstructions that almost reached the quality of the ones with direct sampling and acceleration $\approx 5.5$.

Loss guidance proved to be a powerful tool for including DDPMs into iterative reconstruction schemes and resampling allowed for the increase of iteration steps, that was helpful for the most challenging reconstruction tasks. Since the focus of this work was on finding appropriate conditioning methods, using a set of dedicated test images was not the first priority, but future work should investigate how well this method generalizes to unseen images. The uncertainty of the predictions was also not investigated in this work and since loss guidance coincides with MAP estimation, it might be interesting to introduce additional regularizers that can lower the uncertainty. Finally, models capable of higher resolution should be trained and evaluated. The highest resolution used in this work was $128\times 128$ and section~\ref{sec:networkarch} already presented ideas how fully convolutional architectures could be trained to generalize on higher image resolutions.