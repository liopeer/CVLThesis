%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Materials and Methods}
\section{Image Guided Diffusion}
Both, Choi et al. and Lugmayr et al. make use of unconditional DDPMs for image-guided diffusion for the tasks of image translation in the former and in-painting in the latter.~\autocite{choi2021ilvr,lugmayr2022repaint} Similarly, classifier guidance or CLIP-guidance can be used to condition unconditional DDPMs to produce samples of a specific class or to match a prompt.~\autocite{dhariwal2021diffusion} Both approaches can be combined into a flexible framework that allows the reverse diffusion process to be conditioned on any data consistency term.

\subsection{MAP Estimation for Inverse Problems}
Image reconstruction tasks, that make use of a prior over the desired reconstructed image, can be formulated as a MAP (maximum-a-posteriori) estimation problem
\begin{equation}
    \hat{x}_{MAP} = \argmax_x p(x|s) = \argmax_x \frac{p(s|x)p(x)}{p(s)}
\end{equation}
where $s \sim p(s)$ is the evidence that is provided by the measured signal, $p(x)$ is a prior on the desired reconstruction and the likelihood term $p(s|x)$ enforces a data consistency between the measured signal and the true distribution. Since maximizing $p(x|s)$ is the same as maximizing $\log(x|s)$ and $p(s)$ is independent of $\theta$, we can separate the product into a sum.
\begin{align}
    \hat{x}_{MAP} & = \argmax_x \log p(x|s)             \\ & = \argmax_x \log{\frac{p(s|x)p(x)}{p(s)}} \\
                  & = \argmax_x \log p(s|x)p(x)         \\
                  & = \argmax_x \log p(s|x) + \log p(x)
\end{align}
Such problems can be optimized using iterative optimization schemes such as gradient ascent $x_{t+1} = x_{t} + \lambda \nabla_{x} f(x, s)$, with $\lambda$ being the step length.
\begin{align}
    \label{eq:mapestimation}
    x_{i+1} = x_{i} + \nabla_{x_i} \log p(s|x_i) + \nabla_{x_i} \log p(x_i)
\end{align}
Maximizing $p(s|x)$ and $p(x)$ is in practice usually reformulated as a minimization problem that optimizes for smallest error between prediction and acquisition $\mathcal{L}(s, x)$ and enforces certain regularizers (priors) on $x$ by minimizing $\mathcal{R}(x)$. An example for MRI reconstruction could include minimizing a mean-squared-error between predicted k-space $\mathcal{F}(x)$ and acquired k-space $s$, while also minimizing the total variation in the image.~\autocite{RUDIN1992259}
\begin{equation}
    \hat{x}_{MAP} = \argmin_x \mathcal{L}(s, x) + \mathcal{R}(x) = \argmin_x \frac{1}{n}||\mathcal{F}(x) - s||_2^2 + TV(x)
\end{equation}

\subsection{DDPMs as Priors}
DDPMs approximate a data distribution over training images $p(x)$ and by the score-based formulation, they do so by learning to approximate gradients of this marginal likelihood.~\autocite{song2020generative} Sampling a DDPM therefore equals to starting in a random position and taking gradient ascent steps of the in the direction of maximizing $p(x)$ or $\log p(x)$.
\begin{equation}
    \label{eq:ddpmiteration}
    x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t)
\end{equation}
Comparing this to Eq.~\ref{eq:mapestimation} it is easy to see that this is the same as maximizing for a prior in a reconstruction task and we can introduce a data consistency that works similarly to classifier-guidance~\autocite{dhariwal2021diffusion}, but makes the reverse diffusion process converge to acquired data instead of an easily-classified image.
\begin{align}
    \hat{x} & = \argmax_x \log p(x) + \log p_{\theta}(c|x)        &  & \text{(classifier-guidance)}       \\
            & = \argmax_x \log p(x) + \log p(s|x)                 &  & \text{(data-consistency guidance)} \\
            & = \argmax_x \log p(x) + \argmin_x \mathcal{L}(s, x) &  & \text{(for $\mathcal{L} \geq 0$)}  \\
\end{align}
The constraint $\mathcal{L} \geq 0$ is true for the usual distance-based loss functions like mean-squared-error or the $L_1$ loss. A step in the iterative process has the following form and this algorithm will from now on be termed \textit{loss-guidance}.
\begin{equation}
    x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t) - \nabla_{x_t} \mathcal{L}(s, x)
\end{equation}
The formulation used for the task of reconstructing undersampled MRI used an MSE loss on the predicted and acquired k-space
\begin{equation}
    x_{t-1} = x_{t} + \nabla_{x_t} \log p(x_t) - g \cdot \nabla_{x_t} \frac{1}{\sum_n \mathcal{M}}||\mathcal{M} \circ \mathcal{F}(x_t) - s_0||_2^2
\end{equation}
where $g$ will be termed the \textit{guidance factor} and the MSE is is not scaled by the number of pixels in the image, but by the number of non-zero elements of the mask. This is done in order to compare guidance factors among masks with different accelerations. The guidance factor can be used to balance adherence of the outcome between prior and data consistency.

\section{Frequency Replacement}
As already stated in~\ref{sec:imageguidance}, Choi et al. guide the diffusion process by substituting low frequency content of a desired latent representation with the low-frequencies of the predicted latent space. Since they use linear filters, we are free to reformulate as follows
\begin{align}
    \label{eq:ilvr}
    x_{t} & = \phi(s_{t}) + (I - \phi) (\hat{x}_{t})        \\
          & = \hat{x}_{t} + \phi(s_{t}) - \phi(\hat{x}_{t}) \\
          & = \hat{x}_{t} + \phi(s_{t} - \hat{x}_{t})
\end{align}
where $\phi$ is a linear filter operation and $s_t$ is obtained by using the forward process on the target image.~\autocite{choi2021ilvr} With the knowledge of the gradient of the MSE
\begin{align}
    \text{MSE}              & = \frac{1}{N} (x - s)^T (x - s) \\
    \nabla_{x_t} \text{MSE} & = \frac{2}{N} (x - s)
\end{align}
the frequency replacement can also be interpreted as locally approximating the gradient of the $\nabla_{x}(\phi(s) - \phi(x_{t}))|_{s=s_t}$ and taking a step in that direction, which would correspond to the loss-guidance formulation derived earlier.

Similarly, Lugmayr et al. use a replacement strategy, which we can reformulate to the structure from Choi et al.
\begin{align}
    \label{eq:repaint}
    x_{t} & = \mathcal{M}(s_t) + \mathcal{M}^{-1}(\hat{x}_t)        \\
          & = \mathcal{M}(s_t) + (I - \mathcal{M})(\hat{x}_t)       \\
          & = \hat{x}_t - \mathcal{M}(\hat{x}_t) + \mathcal{M}(s_t) \\
          & = \hat{x}_t + \mathcal{M}(s_t - \hat{x}_t)
\end{align}
Applying these two approaches to the problem of MRI reconstruction requires calculating $s_t$ from $s_0$ which can be done in image space as $s_t = \mathcal{F}^{-1} \circ \mathcal{M} \circ \mathcal{F} (\sqrt{\bar{\alpha}_t} \mathcal{F}^{-1}(s_0) + \sqrt{1-\bar{\alpha}_t} \epsilon)$, or directly in k-space as $s_t = \mathcal{F}^{-1} \circ \mathcal{M} (\sqrt{\bar{\alpha}_t} s_0 + \sqrt{\frac{1-\bar{\alpha}_t}{2}} \epsilon)$ as experimentally derived. The scaling of the noise variance with factor $\frac{1}{2}$ was experimentally found and can be verified in Fig.~\ref{fig:kspacedistribution}. The complete formulation of the update step is therefore
\begin{align}
    x_{t} & = \hat{x}_t + \mathcal{F}^{-1}\circ\mathcal{M}\circ\mathcal{F}(\hat{x}_t) - \mathcal{F}^{-1} \circ \mathcal{M} \circ \mathcal{F} (s_t) \\
          & = \hat{x}_t + \mathcal{F}^{-1}\left(\mathcal{M}\circ\mathcal{F}(\hat{x}_t) - \mathcal{M} \circ \mathcal{F} (s_t)\right)                \\
\end{align}
which makes use of the linearity of the Fourier transform.

\section{Network Architecture}
The neural network is responsible for predicting the noise in an image and the UNet architecture has proven useful for estimating the noise in natural images, which is the context where DDPMs usually operate.~\autocite{ronneberger2015unet,ho2020denoising} The UNet implementation which was used in most experiments of this work is closely related to the original implementation by Ronneberger et al., while most other works use more sophisticated architectures that include Transformer-inspired self-attention layers for better global context awareness of the model and residual connections for faster convergence.~\autocite{vaswani2017attention,he2015deep} Saharia et al. did ablation studies on the self-attention layers and tried to replace them with other methods, such as local self-attention or dilated convolutions, but showed that the global self-attention increased both, mode coverage of the data distribution as well as sample fidelity.~\autocite{saharia2022palette} Fully convolutional architectures have the advantage that, if trained appropriately, they can generalize to different image resolutions. Such training could include random crops and downsamplings of the original images, which was the initial motivation for going with a fully-convolutional architecture. While self-attention layers were included at a later point, the additional computational cost made it difficult to reach convergence in a resonable time. Therefore the best network checkpoint, that was used in the conditioning studies of~\ref{sec:experimentsandresults}, uses the fully convolutional architecture as presented in Fig.~\ref{fig:unetconv}. This architecture sequentially increases the channels and decreases the resolution with a factor of 2 in the encoder and then upsamples the outputs of this bottleneck by incorporating additional local information through the use of skip-connections. The network is conditioned on the timestep by broadcasting a linear embedding of the Transformer-style time encoding (see Fig.~\ref{fig:timeencoding}) onto the feature dimension (channels).~\autocite{vaswani2017attention} The exact specifications can be found in Table~\ref{tab:unetlayers} and the main differences to the UNet architectures used in the DDPM literature are the following:
\begin{description}
    \item[No self-attention layers] Global self-attention increased the computational cost and often led to non-convergence of the network.
    \item[Batch norm instead of layer norm or group norm] Batch norm was not observed to cause any issues, even when training on small mini-batches, therefore it was left as it was.
    \item[No residual connections] Most other literature makes use of residual connections in the double convolutions of the encoder and decoder.
\end{description}
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{images/unet.png}
    \caption[Fully Convolutional UNet]{Fully Convolutional UNet architecture: An input convolution increases the channels to the number of \textit{base channels}. Consecutively a variable number of encoder blocks applies double convolutions and max pooling, before the decoder block applies transpose convolutions for upsampling and double convolutions to incorporate information from the skip connections. At the end, the output convolution maps the output back to the original number of input channels.}
    \label{fig:unetconv}
\end{figure}

\begin{table}
    \centering
    \caption[Overview over UNet Architecture]{Overview over UNet Architecture: The most successful architecture used in this work was a fully-convolutional UNet.}
    \label{tab:unetlayers}
    \begin{tabular}{l l}
        architecture part              & specification                                                                     \\
        \hline\hline
        base channels ($ch$)           & $2^n$                                                                             \\
        \hline base resolution ($res$) & $2^x \times 2^m$                                                                  \\ \hline
        convolutional block            & convolution                                                                       \\ & batchnorm\\ & activation\\ & dropout                         \\
        \hline encoder block           & convolutional block ($ch \rightarrow ch\times 2$)                                 \\ & convolutional block ($ch\times 2 \rightarrow ch\times 2$) \\ & max pool ($res\rightarrow res/ 2$) \\
        \hline decoder block           & transpose convolution ($res\times 2\rightarrow res$, $ch\times 2 \rightarrow ch$) \\ & batchnorm\\ & activation\\ & dropout \\ & skip connection stack ($ch \rightarrow ch\times 2$) \\ & convolutional block ($ch\times 2 \rightarrow ch$)\\
        \hline bottleneck              & convolutional block ($ch \rightarrow ch\times 2$)
    \end{tabular}
\end{table}

\begin{figure}
    \centering
    \
\end{figure}

\section{Slowing Down, Short-Grained Resampling \& Long-Grained Resampling}
Various approaches exist to give the reverse diffusion process more time to converge to a meaninful final prediction.

\section{Datasets}
Introductory experiments were conducted on low-resolution datasets in order to debug the model implementation and determine the best training strategies. These datasets were the well-known MNIST and CIFAR10~\autocite{mnist,cifar} in resolutions of $28\times28$ and $32\times32$ pixels respectively. Since the encoder stack relies on image resolutions of $2^n\times2^k$ with $n,k\in \mathbb{N}$, the MNIST images were upscaled to an equal $32\times32$ resolution. The MNIST dataset is a dataset containing 60'000 training images of handwritten digits 0 to 9 and CIFAR10 contains 50'000 training images distributed over 10 classes like airplane, bird, cat, etc.

The main dataset used in the experiments were the RSS (root sum of squares) reconstructions from the brain dataset in fastMRI.~\autocite{zbontar2018fastMRI} FastMRI is a collection of several MRI datasets, a large dataset of multi-coil brain scans among them. In addition to the raw multi-coil data, RSS reconstructions, combining the coils by using estimates of the sensitivity maps, are also available. Those reconstructions have very high quality and therefore this dataset provides a strong basis for useage as a prior in the reconstruction task. The RSS reconstructions provide a total of 60'090 slices of resolution $320\times320$ pixels and models were trained on downsampled versions of $256\times 256$, $128\times 128$ and $64\times 64$ pixels.

While the authors of fastMRI suggest equally-spaced masks with a fixed center fraction for brain images~\autocite{zbontar2018fastMRI}, the masks used in this work have fixed center fractions, but are randomly sampled for the higher frequencies. Three masks, and the effect they have on the samples, are shown in Fig.~\ref{fig:kspacemasking}. These masks are similar to the ones used in the experimental part.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/corruption_mask.png}
    \caption[K-Space Undersampling]{K-Space Undersampling: a) Sample of a K-Space undersampling mask with center fraction of 0.1 and a probability of 0.25 for the other frequencies, giving an effective acceleration of $\approx 3.12$. b) Effect of K-Space undersampling on samples from fastMRI dataset. While the low-frequency content is visibly intact, the undersampling of the higher frequencies causes aliasing artifacts, manifested as lines on the samples. These lines hint at the true aliases that would be generated for image space undersampling instead of randomized k-space undersampling.}
    \label{fig:kspacemasking}
\end{figure}


\section{Software Package}
In order to fully understand DDPMs it was decided to implement them from scratch instead of using repositories provided by the literature~\autocite{nichol2021improved} or by packages such as the Huggingface Diffusers library.~\autocite{huggingfacediffusers} The created repository is publicly accessible via GitHub and includes automatic documentation generation using Sphinx and GitHub Actions. Notable is also the useage of jaxtyping~\autocite{jaxtyping}, a library for type hinting tensor shapes. The repository and the documentation are accessible under the following links:
\begin{center}
    \hyperlink{https://github.com/liopeer/diffusionmodels}{https://github.com/liopeer/diffusionmodels}\\
    \hyperlink{https://liopeer.github.io/diffusionmodels/}{https://liopeer.github.io/diffusionmodels/}
\end{center}
The software package is based on PyTorch~\autocite{paszke2019pytorch} and provides model architectures as well as training utilities. These utilities include the possibility for distributed training, training logging and checkpointing, and mixed-precision training, implemented using the following frameworks.
\begin{description}
    \item[Weights \& Biases] provides an API that allows logging the model training via their website (\hyperlink{www.wandb.ai}{https://wandb.ai/}). The tool is free for students and academic researchers and automatically logs model configuration, gradients and hardware parameters in addition to user-specified logs, such as sample images, losses and inference times. When using git for versioning it also logs the most recent git commit, allowing to resume model training or to rerun an experiment with exactly the same code. When training models over several days it was very convenient to be able to observe the process from the smartphone and look at samples generated by the model.~\autocite{wandb}
    \item[PyTorch DDP (DistributedDataParallel)] parallelizes model training by launching individual processes for each GPU, or it can even launch processes across different machines. Separate processes are necessary in order to enable true parallelism that avoids Python GIL (global interpreter lock). During initialization, the model is copied across the different GPUs and during training only the gradients are synchronized and averaged across the GPUs, therefore the optimizers essentially train a local model per each process. Gradient synchronization is automatically invoked by calling \lstinline{loss.backward()}, but can be avoided by including forward and backward in the \lstinline{no_sync()} content manager, which is useful when using gradient accumulation over several micro-batches, for which the gradient synchronization would create unnecessary overhead. As part of DDP, PyTorch also offers \lstinline{DistributedSampler} (to be used with \lstinline{DataLoader}), which splits mini-batches into micro-batches and assigns them to the respective processes. For models that use batch normalization layers, DDP also offers the module \lstinline{SyncBatchNorm} and a function to recursively change all batch normalization layers to synchronized batch normalization. Synchronizing the batch normalization might be important for small micro-batch sizes or when the number of GPUs changes during training (e.g. continuing from a checkpoint).
    \item[PyTorch AMP (Automatic Mixed Precision)] provides a context manager and a function decorator that will convert certain operations to half-precision (16 bit), which gives a significant speedup for linear layers or convolutions, but keeps high precision for operations such as reductions. Half precision training might lead to underflow of gradients, because of the reduced value range and can be avoided by scaling the loss and therefore the gradients, while also inversely scaling the update step. AMP provides the \lstinline{GradScaler} class for this purpose.
\end{description}